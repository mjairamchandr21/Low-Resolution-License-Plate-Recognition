{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a666d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create submission zip\n",
    "predictions_file = '/content/output/predictions.txt'\n",
    "submission_zip = '/content/output/submission.zip'\n",
    "\n",
    "print(\"ðŸ“¦ Creating submission.zip...\")\n",
    "\n",
    "with zipfile.ZipFile(submission_zip, 'w') as zipf:\n",
    "    zipf.write(predictions_file, arcname='predictions.txt')\n",
    "\n",
    "print(f\"âœ… Submission file created: {submission_zip}\")\n",
    "print(f\"   File size: {os.path.getsize(submission_zip) / 1024:.2f} KB\")\n",
    "\n",
    "# Verify zip contents\n",
    "print(\"\\nðŸ“‹ ZIP file contents:\")\n",
    "with zipfile.ZipFile(submission_zip, 'r') as zipf:\n",
    "    for file_info in zipf.filelist:\n",
    "        print(f\"   {file_info.filename} ({file_info.file_size} bytes)\")\n",
    "\n",
    "# Copy to Google Drive\n",
    "print(\"\\nðŸ’¾ Copying to Google Drive...\")\n",
    "import shutil\n",
    "drive_submission = '/content/drive/MyDrive/LPR_Project/submission.zip'\n",
    "shutil.copy(submission_zip, drive_submission)\n",
    "print(f\"âœ… Submission saved to Google Drive: {drive_submission}\")\n",
    "\n",
    "# Also copy predictions.txt for reference\n",
    "drive_predictions = '/content/drive/MyDrive/LPR_Project/predictions.txt'\n",
    "shutil.copy(predictions_file, drive_predictions)\n",
    "print(f\"âœ… Predictions saved to Google Drive: {drive_predictions}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ SUBMISSION READY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… Submission file: {submission_zip}\")\n",
    "print(f\"âœ… Ready to upload to CodaBench\")\n",
    "print(\"\\nInstructions:\")\n",
    "print(\"1. Download submission.zip from Google Drive\")\n",
    "print(\"2. Go to CodaBench competition page\")\n",
    "print(\"3. Navigate to 'My Submissions' tab\")\n",
    "print(\"4. Upload submission.zip\")\n",
    "print(\"5. Monitor leaderboard for results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d498f75",
   "metadata": {},
   "source": [
    "## 8. Create Submission ZIP File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(model, inference_dataset, device, output_path='/content/output/predictions.txt'):\n",
    "    \"\"\"Generate submission predictions for all tracks\"\"\"\n",
    "    \n",
    "    predictions_list = []\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"ðŸš€ Generating predictions for {len(inference_dataset)} tracks...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(len(inference_dataset))):\n",
    "            track_id, images = inference_dataset[idx]\n",
    "            \n",
    "            if not images:\n",
    "                continue\n",
    "            \n",
    "            # Get predictions from all images\n",
    "            predictions, confidences = predict_track(model, images, device)\n",
    "            \n",
    "            # Aggregate predictions\n",
    "            final_pred, final_conf = aggregate_predictions(predictions, confidences)\n",
    "            \n",
    "            if final_pred.strip():  # Only save non-empty predictions\n",
    "                predictions_list.append({\n",
    "                    'track_id': track_id,\n",
    "                    'plate_text': final_pred,\n",
    "                    'confidence': final_conf\n",
    "                })\n",
    "    \n",
    "    # Save predictions to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        for pred in predictions_list:\n",
    "            line = f\"{pred['track_id']},{pred['plate_text']};{pred['confidence']:.4f}\"\n",
    "            f.write(line + '\\n')\n",
    "    \n",
    "    print(f\"âœ… Predictions saved to {output_path}\")\n",
    "    print(f\"   Total predictions: {len(predictions_list)}\")\n",
    "    \n",
    "    return predictions_list\n",
    "\n",
    "\n",
    "# Generate submission\n",
    "inference_dataset = InferenceDataset(DATA_PATH, num_images=5)\n",
    "predictions = generate_submission(model, inference_dataset, DEVICE)\n",
    "\n",
    "# Display first 10 predictions\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "for pred in predictions[:10]:\n",
    "    print(f\"  {pred['track_id']},{pred['plate_text']};{pred['confidence']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95283988",
   "metadata": {},
   "source": [
    "## 7. Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ae2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for inference\n",
    "model.load_state_dict(best_checkpoint)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Best model loaded for inference\")\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \"\"\"Dataset for inference on test tracks\"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, num_images=5):\n",
    "        self.samples = []\n",
    "        self.num_images = num_images\n",
    "        self._load_samples(data_root)\n",
    "    \n",
    "    def _load_samples(self, data_root):\n",
    "        \"\"\"Load all track paths\"\"\"\n",
    "        for scenario in os.listdir(data_root):\n",
    "            scenario_path = os.path.join(data_root, scenario)\n",
    "            if not os.path.isdir(scenario_path):\n",
    "                continue\n",
    "            \n",
    "            for layout in os.listdir(scenario_path):\n",
    "                layout_path = os.path.join(scenario_path, layout)\n",
    "                if not os.path.isdir(layout_path):\n",
    "                    continue\n",
    "                \n",
    "                for track in os.listdir(layout_path):\n",
    "                    track_path = os.path.join(layout_path, track)\n",
    "                    if not os.path.isdir(track_path):\n",
    "                        continue\n",
    "                    \n",
    "                    ann_path = os.path.join(track_path, 'annotations.json')\n",
    "                    if os.path.exists(ann_path):\n",
    "                        self.samples.append({\n",
    "                            'track_path': track_path,\n",
    "                            'track_id': track,\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        track_path = sample['track_path']\n",
    "        track_id = sample['track_id']\n",
    "        \n",
    "        images = []\n",
    "        for i in range(1, self.num_images + 1):\n",
    "            img_path = os.path.join(track_path, f'lr-00{i}.png')\n",
    "            if os.path.exists(img_path):\n",
    "                image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                image = cv2.resize(image, (128, 32))\n",
    "                image = image.astype(np.float32) / 255.0\n",
    "                image = torch.from_numpy(image).unsqueeze(0)\n",
    "                images.append(image)\n",
    "        \n",
    "        return track_id, images\n",
    "\n",
    "\n",
    "def predict_track(model, images, device):\n",
    "    \"\"\"Predict on multiple images from a track\"\"\"\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image in images:\n",
    "            image = image.to(device)\n",
    "            logits = model(image.unsqueeze(0))  # Add batch dimension\n",
    "            \n",
    "            # Get prediction\n",
    "            output = torch.argmax(logits, dim=2)  # (1, 32)\n",
    "            \n",
    "            # Greedy decoding\n",
    "            prev, seq = -1, []\n",
    "            for t in range(output.size(1)):\n",
    "                val = output[0, t].item()\n",
    "                if val != prev and val != 0:\n",
    "                    seq.append(val)\n",
    "                prev = val\n",
    "            \n",
    "            pred_text = decode_to_string(seq)\n",
    "            \n",
    "            # Calculate confidence as average softmax probability\n",
    "            softmax = torch.softmax(logits, dim=2)  # (NUM_CLASSES, 1, 32)\n",
    "            max_probs = torch.max(softmax, dim=0)[0]  # (1, 32)\n",
    "            confidence = max_probs.mean().item()\n",
    "            \n",
    "            predictions.append(pred_text)\n",
    "            confidences.append(confidence)\n",
    "    \n",
    "    return predictions, confidences\n",
    "\n",
    "\n",
    "def aggregate_predictions(predictions, confidences):\n",
    "    \"\"\"Aggregate predictions from multiple images using confidence-based voting\"\"\"\n",
    "    if not predictions:\n",
    "        return \"\", 0.0\n",
    "    \n",
    "    # Filter out empty predictions\n",
    "    valid_preds = [(p, c) for p, c in zip(predictions, confidences) if p.strip()]\n",
    "    \n",
    "    if not valid_preds:\n",
    "        return \"\", 0.0\n",
    "    \n",
    "    # Majority voting with confidence weighting\n",
    "    from collections import Counter\n",
    "    pred_texts = [p for p, _ in valid_preds]\n",
    "    \n",
    "    # Count occurrences\n",
    "    counter = Counter(pred_texts)\n",
    "    most_common = counter.most_common(1)[0][0]\n",
    "    \n",
    "    # Get average confidence for most common\n",
    "    avg_confidence = np.mean([\n",
    "        c for p, c in valid_preds if p == most_common\n",
    "    ])\n",
    "    \n",
    "    return most_common, avg_confidence\n",
    "\n",
    "# Test inference on a single sample\n",
    "print(\"\\nðŸ” Testing inference on a single sample...\")\n",
    "inference_dataset = InferenceDataset(DATA_PATH, num_images=5)\n",
    "track_id, images = inference_dataset[0]\n",
    "print(f\"Track ID: {track_id}\")\n",
    "print(f\"Number of images: {len(images)}\")\n",
    "\n",
    "predictions, confidences = predict_track(model, images, DEVICE)\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"Confidences: {[f'{c:.4f}' for c in confidences]}\")\n",
    "\n",
    "final_pred, final_conf = aggregate_predictions(predictions, confidences)\n",
    "print(f\"Final prediction: {final_pred}\")\n",
    "print(f\"Final confidence: {final_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb23d64",
   "metadata": {},
   "source": [
    "## 6. Inference & Prediction Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "epochs = 25\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = {'train_loss': [], 'val_acc': []}\n",
    "best_val_acc = 0.0\n",
    "best_checkpoint = None\n",
    "\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "print(f\"   Epochs: {epochs}\")\n",
    "print(f\"   Learning rate: {lr}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_acc = validate(model, val_loader, DEVICE)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Accuracy: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_checkpoint = model.state_dict()\n",
    "        checkpoint_path = f'/content/checkpoints/model_best.pt'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"âœ… Best model saved (accuracy: {val_acc*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Training completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['val_acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/output/training_history.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for DataLoader\"\"\"\n",
    "    images, labels, lengths, texts = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    targets = torch.cat(labels)\n",
    "    target_lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return images, targets, target_lengths, texts\n",
    "\n",
    "\n",
    "def greedy_decoder(output):\n",
    "    \"\"\"Decode CTC output using greedy decoding\"\"\"\n",
    "    # output: (T, B, C)\n",
    "    output = torch.argmax(output, dim=2)  # (T, B)\n",
    "    decoded = []\n",
    "    \n",
    "    for b in range(output.size(1)):\n",
    "        prev, seq = -1, []\n",
    "        for t in range(output.size(0)):\n",
    "            val = output[t, b].item()\n",
    "            if val != prev and val != 0:  # Skip blanks and consecutive duplicates\n",
    "                seq.append(val)\n",
    "            prev = val\n",
    "        decoded.append(seq)\n",
    "    \n",
    "    return decoded\n",
    "\n",
    "\n",
    "def decode_to_string(seq):\n",
    "    \"\"\"Convert indices to string\"\"\"\n",
    "    return \"\".join(idx_to_char.get(i, '') for i in seq if i in idx_to_char)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    \"\"\"Validate model on validation set\"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, target_lengths, texts in val_loader:\n",
    "            images = images.to(device)\n",
    "            logits = model(images)  # (NUM_CLASSES, B, 32)\n",
    "            decoded = greedy_decoder(logits)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            idx = 0\n",
    "            for i, seq in enumerate(decoded):\n",
    "                gt = decode_to_string(targets[idx:idx+target_lengths[i]].tolist())\n",
    "                pred = decode_to_string(seq)\n",
    "                \n",
    "                if gt.strip() == pred.strip():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "                idx += target_lengths[i]\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for images, targets, target_lengths, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(images)  # (NUM_CLASSES, B, 32)\n",
    "        \n",
    "        # Calculate input lengths (sequence length from model)\n",
    "        input_lengths = torch.full(\n",
    "            (logits.size(1),), logits.size(0), dtype=torch.long, device=device\n",
    "        )\n",
    "        \n",
    "        # CTC Loss\n",
    "        loss = criterion(logits, targets, input_lengths, target_lengths)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_dataset = LPRDataset(DATA_PATH, augment=True, use_hr=False)\n",
    "train_size = int(0.95 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"âœ… Data loaders created\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Validation samples: {len(val_data)}\")\n",
    "print(f\"   Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5fa9f6",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(torch.nn.Module):\n",
    "    \"\"\"Convolutional Recurrent Neural Network for License Plate Recognition\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, dropout=0.3):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        # CNN Feature Extractor\n",
    "        self.cnn = torch.nn.Sequential(\n",
    "            # Layer 1: 1 -> 64 channels\n",
    "            torch.nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2),  # 32x128 -> 16x64\n",
    "            \n",
    "            # Layer 2: 64 -> 128 channels\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2),  # 16x64 -> 8x32\n",
    "            \n",
    "            # Layer 3: 128 -> 256 channels\n",
    "            torch.nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2, 1)),  # 8x32 -> 4x32\n",
    "            \n",
    "            # Layer 4: 256 -> 512 channels\n",
    "            torch.nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2, 1))  # 4x32 -> 2x32\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size=512 * 2,  # 512 channels Ã— 2 height\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if dropout > 0 else 0\n",
    "        )\n",
    "        \n",
    "        # Classifier head\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(512, num_classes)  # 256 * 2 (bi-directional)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 32, 128)\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        features = self.cnn(x)  # (B, 512, 2, 32)\n",
    "        \n",
    "        # Reshape for RNN: (B, L, C) where L is sequence length\n",
    "        b, c, h, w = features.size()\n",
    "        features = features.permute(0, 3, 1, 2)  # (B, 32, 512, 2)\n",
    "        features = features.reshape(b, w, c * h)  # (B, 32, 1024)\n",
    "        \n",
    "        # RNN processing\n",
    "        rnn_out, _ = self.rnn(features)  # (B, 32, 512)\n",
    "        \n",
    "        # Classifier\n",
    "        logits = self.fc(rnn_out)  # (B, 32, NUM_CLASSES)\n",
    "        logits = logits.permute(2, 0, 1)  # (NUM_CLASSES, B, 32) for CTC\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "model = CRNN(num_classes=NUM_CLASSES, dropout=0.3)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ… CRNN Model initialized\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977a147",
   "metadata": {},
   "source": [
    "## 4. Model Architecture: CRNN (CNN + RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character mapping for OCR\n",
    "CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "char_to_idx = {c: i + 1 for i, c in enumerate(CHARS)}\n",
    "idx_to_char = {i + 1: c for i, c in enumerate(CHARS)}\n",
    "NUM_CLASSES = len(CHARS) + 1  # +1 for CTC blank\n",
    "\n",
    "print(f\"âœ… Character set: {CHARS}\")\n",
    "print(f\"âœ… NUM_CLASSES: {NUM_CLASSES}\")\n",
    "\n",
    "class LPRDataset(Dataset):\n",
    "    \"\"\"License Plate Recognition Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, augment=True, scenario_filter=None, use_hr=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_root: path to train/ folder\n",
    "            augment: if True, apply random augmentations\n",
    "            scenario_filter: filter by scenario (e.g., \"Scenario-B\")\n",
    "            use_hr: if True, use HR images; else use LR images\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.data_root = data_root\n",
    "        self.augment = augment\n",
    "        self.scenario_filter = scenario_filter\n",
    "        self.use_hr = use_hr\n",
    "        self._load_samples()\n",
    "    \n",
    "    def _load_samples(self):\n",
    "        \"\"\"Load all sample paths\"\"\"\n",
    "        for scenario in os.listdir(self.data_root):\n",
    "            scenario_path = os.path.join(self.data_root, scenario)\n",
    "            if not os.path.isdir(scenario_path):\n",
    "                continue\n",
    "            \n",
    "            if self.scenario_filter and scenario != self.scenario_filter:\n",
    "                continue\n",
    "            \n",
    "            for layout in os.listdir(scenario_path):\n",
    "                layout_path = os.path.join(scenario_path, layout)\n",
    "                if not os.path.isdir(layout_path):\n",
    "                    continue\n",
    "                \n",
    "                for track in os.listdir(layout_path):\n",
    "                    track_path = os.path.join(layout_path, track)\n",
    "                    if not os.path.isdir(track_path):\n",
    "                        continue\n",
    "                    \n",
    "                    ann_path = os.path.join(track_path, 'annotations.json')\n",
    "                    if os.path.exists(ann_path):\n",
    "                        self.samples.append({\n",
    "                            'track_path': track_path,\n",
    "                            'track_id': track,\n",
    "                            'scenario': scenario,\n",
    "                            'layout': layout\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        track_path = sample['track_path']\n",
    "        \n",
    "        # Load annotation\n",
    "        ann_path = os.path.join(track_path, 'annotations.json')\n",
    "        with open(ann_path) as f:\n",
    "            ann = json.load(f)\n",
    "        \n",
    "        plate_text = ann.get('plate_text', '')\n",
    "        \n",
    "        # Select image (first LR or HR image)\n",
    "        img_prefix = 'hr-001.png' if self.use_hr else 'lr-001.png'\n",
    "        img_path = os.path.join(track_path, img_prefix)\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (128, 32))  # Resize to fixed size\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Apply augmentation if training\n",
    "        if self.augment and np.random.rand() > 0.5:\n",
    "            image = self._augment(image)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image = torch.from_numpy(image).unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        # Convert plate text to indices\n",
    "        label = [char_to_idx.get(c, 0) for c in plate_text if c in char_to_idx]\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        label_length = len(label)\n",
    "        \n",
    "        return image, label_tensor, label_length, plate_text\n",
    "    \n",
    "    def _augment(self, image):\n",
    "        \"\"\"Apply random augmentations\"\"\"\n",
    "        # Random brightness\n",
    "        if np.random.rand() > 0.5:\n",
    "            brightness = np.random.uniform(0.8, 1.2)\n",
    "            image = np.clip(image * brightness, 0, 1)\n",
    "        \n",
    "        # Random contrast\n",
    "        if np.random.rand() > 0.5:\n",
    "            contrast = np.random.uniform(0.8, 1.2)\n",
    "            image = np.clip((image - 0.5) * contrast + 0.5, 0, 1)\n",
    "        \n",
    "        # Random noise\n",
    "        if np.random.rand() > 0.7:\n",
    "            noise = np.random.normal(0, 0.02, image.shape)\n",
    "            image = np.clip(image + noise, 0, 1)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Test dataset loading\n",
    "train_dataset = LPRDataset(DATA_PATH, augment=False)\n",
    "print(f\"\\nâœ… Dataset loaded: {len(train_dataset)} samples\")\n",
    "\n",
    "# Display a sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"Sample image shape: {sample[0].shape}\")\n",
    "print(f\"Sample label: {sample[3]} (indices: {sample[1].tolist()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10cbff",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308844a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "def visualize_samples(data_path, num_samples=3):\n",
    "    \"\"\"Visualize sample LR and HR images\"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(10, 4*num_samples))\n",
    "    \n",
    "    sample_count = 0\n",
    "    for scenario in os.listdir(data_path):\n",
    "        scenario_path = os.path.join(data_path, scenario)\n",
    "        if not os.path.isdir(scenario_path):\n",
    "            continue\n",
    "        \n",
    "        for layout in os.listdir(scenario_path):\n",
    "            layout_path = os.path.join(scenario_path, layout)\n",
    "            if not os.path.isdir(layout_path):\n",
    "                continue\n",
    "            \n",
    "            for track in os.listdir(layout_path):\n",
    "                if sample_count >= num_samples:\n",
    "                    break\n",
    "                \n",
    "                track_path = os.path.join(layout_path, track)\n",
    "                if not os.path.isdir(track_path):\n",
    "                    continue\n",
    "                \n",
    "                lr_img = os.path.join(track_path, 'lr-001.png')\n",
    "                hr_img = os.path.join(track_path, 'hr-001.png')\n",
    "                ann_file = os.path.join(track_path, 'annotations.json')\n",
    "                \n",
    "                if os.path.exists(lr_img) and os.path.exists(hr_img):\n",
    "                    lr = cv2.imread(lr_img, cv2.IMREAD_GRAYSCALE)\n",
    "                    hr = cv2.imread(hr_img, cv2.IMREAD_GRAYSCALE)\n",
    "                    \n",
    "                    axes[sample_count, 0].imshow(lr, cmap='gray')\n",
    "                    axes[sample_count, 0].set_title(f\"LR: {lr.shape} (Scenario: {scenario})\")\n",
    "                    axes[sample_count, 0].axis('off')\n",
    "                    \n",
    "                    axes[sample_count, 1].imshow(hr, cmap='gray')\n",
    "                    axes[sample_count, 1].set_title(f\"HR: {hr.shape}\")\n",
    "                    axes[sample_count, 1].axis('off')\n",
    "                    \n",
    "                    if os.path.exists(ann_file):\n",
    "                        with open(ann_file) as f:\n",
    "                            ann = json.load(f)\n",
    "                            plate_text = ann.get('plate_text', 'N/A')\n",
    "                            layout = ann.get('plate_layout', 'N/A')\n",
    "                            fig.text(0.5, 0.02 + sample_count*0.15, \n",
    "                                    f\"Track: {track} | Plate: {plate_text} | Layout: {layout}\",\n",
    "                                    ha='center', fontsize=10)\n",
    "                    \n",
    "                    sample_count += 1\n",
    "            \n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/output/sample_images.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ… Sample images saved to /content/output/sample_images.png\")\n",
    "\n",
    "visualize_samples(DATA_PATH, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration function\n",
    "def explore_dataset(data_path):\n",
    "    \"\"\"Explore dataset structure and statistics\"\"\"\n",
    "    stats = {\n",
    "        'total_tracks': 0,\n",
    "        'scenarios': {},\n",
    "        'plate_layouts': {'Brazilian': 0, 'Mercosur': 0},\n",
    "        'plate_texts': [],\n",
    "        'image_sizes': {'lr': [], 'hr': []}\n",
    "    }\n",
    "    \n",
    "    for scenario in os.listdir(data_path):\n",
    "        scenario_path = os.path.join(data_path, scenario)\n",
    "        if not os.path.isdir(scenario_path):\n",
    "            continue\n",
    "        \n",
    "        stats['scenarios'][scenario] = 0\n",
    "        \n",
    "        for layout in os.listdir(scenario_path):\n",
    "            layout_path = os.path.join(scenario_path, layout)\n",
    "            if layout in ['Brazilian', 'Mercosur']:\n",
    "                stats['plate_layouts'][layout] += 1\n",
    "            \n",
    "            if not os.path.isdir(layout_path):\n",
    "                continue\n",
    "            \n",
    "            for track in os.listdir(layout_path):\n",
    "                track_path = os.path.join(layout_path, track)\n",
    "                if not os.path.isdir(track_path):\n",
    "                    continue\n",
    "                \n",
    "                ann_file = os.path.join(track_path, 'annotations.json')\n",
    "                if os.path.exists(ann_file):\n",
    "                    with open(ann_file) as f:\n",
    "                        ann = json.load(f)\n",
    "                        stats['plate_texts'].append(ann.get('plate_text', ''))\n",
    "                \n",
    "                # Get image sizes\n",
    "                for img_file in os.listdir(track_path):\n",
    "                    if img_file.endswith('.png'):\n",
    "                        img_path = os.path.join(track_path, img_file)\n",
    "                        try:\n",
    "                            img = Image.open(img_path)\n",
    "                            if img_file.startswith('lr-'):\n",
    "                                stats['image_sizes']['lr'].append(img.size)\n",
    "                            elif img_file.startswith('hr-'):\n",
    "                                stats['image_sizes']['hr'].append(img.size)\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                stats['total_tracks'] += 1\n",
    "                stats['scenarios'][scenario] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"ðŸ” Exploring dataset...\")\n",
    "dataset_stats = explore_dataset(DATA_PATH)\n",
    "\n",
    "print(f\"\\nâœ… Dataset Statistics:\")\n",
    "print(f\"Total tracks: {dataset_stats['total_tracks']}\")\n",
    "print(f\"Scenarios: {dataset_stats['scenarios']}\")\n",
    "print(f\"Plate layouts: {dataset_stats['plate_layouts']}\")\n",
    "print(f\"Unique plate texts: {len(set(dataset_stats['plate_texts']))}\")\n",
    "if dataset_stats['image_sizes']['lr']:\n",
    "    print(f\"LR image size: {dataset_stats['image_sizes']['lr'][0]} (samples: {len(dataset_stats['image_sizes']['lr'])})\")\n",
    "if dataset_stats['image_sizes']['hr']:\n",
    "    print(f\"HR image size: {dataset_stats['image_sizes']['hr'][0]} (samples: {len(dataset_stats['image_sizes']['hr'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b93e94",
   "metadata": {},
   "source": [
    "## 2. Data Exploration & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "DRIVE_PATH = '/content/drive/MyDrive/LPR_Project'\n",
    "DATA_PATH = '/content/drive/MyDrive/LPR_Project/data/raw/wYe7pBJ7-train/train'\n",
    "\n",
    "# Create working directories\n",
    "os.makedirs('/content/output', exist_ok=True)\n",
    "os.makedirs('/content/checkpoints', exist_ok=True)\n",
    "\n",
    "print(f\"Drive path: {DRIVE_PATH}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"\\nDirectory structure:\")\n",
    "print(f\"Data exists: {os.path.exists(DATA_PATH)}\")\n",
    "if os.path.exists(DATA_PATH):\n",
    "    print(f\"Contents: {os.listdir(DATA_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"âœ… Using device: {DEVICE}\")\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'torch',\n",
    "    'torchvision',\n",
    "    'opencv-python',\n",
    "    'pillow',\n",
    "    'numpy',\n",
    "    'tqdm',\n",
    "    'scikit-image',\n",
    "    'paddleocr',\n",
    "    'pyyaml',\n",
    "]\n",
    "\n",
    "print(\"Installing packages...\")\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "print(\"âœ… All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63863fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"âœ… Google Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7134d84",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1f6d5",
   "metadata": {},
   "source": [
    "# ICPR 2026: Low-Resolution License Plate Recognition\n",
    "## Complete End-to-End Solution\n",
    "\n",
    "This notebook provides a complete workflow for the ICPR 2026 competition using:\n",
    "- **Super-Resolution**: Upscaling low-res license plate images\n",
    "- **OCR**: Character recognition on enhanced images\n",
    "- **Aggregation**: Combining predictions across multiple frames\n",
    "\n",
    "**Target**: Achieve >60% accuracy on blind test set"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
